{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting git+https://github.com/openai/whisper.git (from -r requirements.txt (line 1))\n",
      "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-q4ua_l8u\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-q4ua_l8u\n",
      "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyannote.audio (from -r requirements.txt (line 2))\n",
      "  Obtaining dependency information for pyannote.audio from https://files.pythonhosted.org/packages/f5/11/611c32f7b7894ba588ade502525d0130f3e731d15f925e9f2a1ae66c8680/pyannote.audio-3.1.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pyannote.audio-3.1.1-py2.py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting pydub (from -r requirements.txt (line 3))\n",
      "  Obtaining dependency information for pydub from https://files.pythonhosted.org/packages/a6/53/d78dc063216e62fc55f6b2eebb447f6a4b0a59f55c8406376f76bf959b08/pydub-0.25.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting speechbox (from -r requirements.txt (line 4))\n",
      "  Obtaining dependency information for speechbox from https://files.pythonhosted.org/packages/32/e8/4cb10f042ea08fd234545e0d386243d2b77f94d3b39ee6432b842242d8c3/speechbox-0.2.1-py3-none-any.whl.metadata\n",
      "  Downloading speechbox-0.2.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numba in /home/ubuntu/.local/lib/python3.10/site-packages (from openai-whisper==20231117->-r requirements.txt (line 1)) (0.59.1)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (from openai-whisper==20231117->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: torch in /home/ubuntu/.local/lib/python3.10/site-packages (from openai-whisper==20231117->-r requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (from openai-whisper==20231117->-r requirements.txt (line 1)) (4.66.2)\n",
      "Collecting more-itertools (from openai-whisper==20231117->-r requirements.txt (line 1))\n",
      "  Obtaining dependency information for more-itertools from https://files.pythonhosted.org/packages/50/e2/8e10e465ee3987bb7c9ab69efb91d867d93959095f4807db102d07995d94/more_itertools-10.2.0-py3-none-any.whl.metadata\n",
      "  Downloading more_itertools-10.2.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: tiktoken in /home/ubuntu/.local/lib/python3.10/site-packages (from openai-whisper==20231117->-r requirements.txt (line 1)) (0.5.2)\n",
      "Requirement already satisfied: triton<3,>=2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from openai-whisper==20231117->-r requirements.txt (line 1)) (2.2.0)\n",
      "Collecting asteroid-filterbanks>=0.4 (from pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for asteroid-filterbanks>=0.4 from https://files.pythonhosted.org/packages/c5/7c/83ff6046176a675e6a1e8aeefed8892cd97fe7c46af93cc540d1b24b8323/asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata\n",
      "  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting einops>=0.6.0 (from pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for einops>=0.6.0 from https://files.pythonhosted.org/packages/29/0b/2d1c0ebfd092e25935b86509a9a817159212d82aa43d7fb07eca4eeff2c2/einops-0.7.0-py3-none-any.whl.metadata\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyannote.audio->-r requirements.txt (line 2)) (0.21.4)\n",
      "Collecting lightning>=2.0.1 (from pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for lightning>=2.0.1 from https://files.pythonhosted.org/packages/49/03/1325c91ec4824ede1102dd1ee240bceb8c5aa2b75c5bec83c59f0bb8da07/lightning-2.2.2-py3-none-any.whl.metadata\n",
      "  Downloading lightning-2.2.2-py3-none-any.whl.metadata (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting omegaconf<3.0,>=2.1 (from pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for omegaconf<3.0,>=2.1 from https://files.pythonhosted.org/packages/e3/94/1843518e420fa3ed6919835845df698c7e27e183cb997394e4a670973a65/omegaconf-2.3.0-py3-none-any.whl.metadata\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pyannote.core>=5.0.0 (from pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for pyannote.core>=5.0.0 from https://files.pythonhosted.org/packages/84/c4/370bc8ba66815a5832ece753a1009388bb07ea353d21c83f2d5a1a436f2c/pyannote.core-5.0.0-py3-none-any.whl.metadata\n",
      "  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting pyannote.database>=5.0.1 (from pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for pyannote.database>=5.0.1 from https://files.pythonhosted.org/packages/1e/c1/e538adc4027ead249320436564587c5c8393e39873985a2e4de52f44fccf/pyannote.database-5.1.0-py3-none-any.whl.metadata\n",
      "  Downloading pyannote.database-5.1.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting pyannote.metrics>=3.2 (from pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for pyannote.metrics>=3.2 from https://files.pythonhosted.org/packages/6c/7d/035b370ab834b30e849fe9cd092b7bd7f321fcc4a2c56b84e96476b7ede5/pyannote.metrics-3.2.1-py3-none-any.whl.metadata\n",
      "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting pyannote.pipeline>=3.0.1 (from pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for pyannote.pipeline>=3.0.1 from https://files.pythonhosted.org/packages/83/42/1bf7cbf061ed05c580bfb63bffdd3f3474cbd5c02bee4fac518eea9e9d9e/pyannote.pipeline-3.0.1-py3-none-any.whl.metadata\n",
      "  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\n",
      "Collecting pytorch-metric-learning>=2.1.0 (from pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for pytorch-metric-learning>=2.1.0 from https://files.pythonhosted.org/packages/e4/e1/3ee45254ca21a3c2bdd3c60e8b0b79c74f177e87a1abeb300ac151f21492/pytorch_metric_learning-2.5.0-py3-none-any.whl.metadata\n",
      "  Downloading pytorch_metric_learning-2.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: rich>=12.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyannote.audio->-r requirements.txt (line 2)) (13.7.1)\n",
      "Collecting semver>=3.0.0 (from pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for semver>=3.0.0 from https://files.pythonhosted.org/packages/9a/77/0cc7a8a3bc7e53d07e8f47f147b92b0960e902b8254859f4aee5c4d7866b/semver-3.0.2-py3-none-any.whl.metadata\n",
      "  Downloading semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting soundfile>=0.12.1 (from pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for soundfile>=0.12.1 from https://files.pythonhosted.org/packages/ad/bd/0602167a213d9184fc688b1086dc6d374b7ae8c33eccf169f9b50ce6568c/soundfile-0.12.1-py2.py3-none-manylinux_2_17_x86_64.whl.metadata\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_17_x86_64.whl.metadata (14 kB)\n",
      "Collecting speechbrain>=0.5.14 (from pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for speechbrain>=0.5.14 from https://files.pythonhosted.org/packages/90/ee/c8669b57ebdbeac0530538725caa02cd226e2623b725f1e216ae59b54a1f/speechbrain-1.0.0-py3-none-any.whl.metadata\n",
      "  Downloading speechbrain-1.0.0-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting tensorboardX>=2.6 (from pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for tensorboardX>=2.6 from https://files.pythonhosted.org/packages/44/71/f3e7c9b2ab67e28c572ab4e9d5fa3499e0d252650f96d8a3a03e26677f53/tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting torch-audiomentations>=0.11.0 (from pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for torch-audiomentations>=0.11.0 from https://files.pythonhosted.org/packages/88/2c/03ffe92c8c28e4511c7f8108decb2065b9ee7c0ee69bfeb66325b2b4d513/torch_audiomentations-0.11.1-py3-none-any.whl.metadata\n",
      "  Downloading torch_audiomentations-0.11.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting torchaudio>=2.0.0 (from pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for torchaudio>=2.0.0 from https://files.pythonhosted.org/packages/73/ce/75bb2a9340a2a8d6fd61fcd1e6e386a75ba2a0fb72a0f1ded18c2b1c4bd1/torchaudio-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata\n",
      "  Downloading torchaudio-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torchmetrics>=0.11.0 (from pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for torchmetrics>=0.11.0 from https://files.pythonhosted.org/packages/f3/0e/cedcb9c8aeb2d1f655f8d05f841b14d84b0a68d9f31afae4af55c7c6d0a9/torchmetrics-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading torchmetrics-1.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: importlib-metadata in /home/ubuntu/.local/lib/python3.10/site-packages (from speechbox->-r requirements.txt (line 4)) (6.11.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from speechbox->-r requirements.txt (line 4)) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/.local/lib/python3.10/site-packages (from asteroid-filterbanks>=0.4->pyannote.audio->-r requirements.txt (line 2)) (4.10.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.13.0->pyannote.audio->-r requirements.txt (line 2)) (2024.2.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.13.0->pyannote.audio->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.13.0->pyannote.audio->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.13.0->pyannote.audio->-r requirements.txt (line 2)) (23.2)\n",
      "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning>=2.0.1->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for lightning-utilities<2.0,>=0.8.0 from https://files.pythonhosted.org/packages/5e/9e/e7768a8e363fc6f0c978bb7a0aa7641f10d80be60000e788ef2f01d34a7c/lightning_utilities-0.11.2-py3-none-any.whl.metadata\n",
      "  Downloading lightning_utilities-0.11.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pytorch-lightning (from lightning>=2.0.1->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for pytorch-lightning from https://files.pythonhosted.org/packages/e9/3c/8d05e269011d43c35b1fb3baca8d88bac6668f6c4bde565ad9ed27000b22/pytorch_lightning-2.2.2-py3-none-any.whl.metadata\n",
      "  Downloading pytorch_lightning-2.2.2-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf<3.0,>=2.1->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: sortedcontainers>=2.0.4 in /home/ubuntu/anaconda3/lib/python3.10/site-packages (from pyannote.core>=5.0.0->pyannote.audio->-r requirements.txt (line 2)) (2.4.0)\n",
      "Requirement already satisfied: scipy>=1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyannote.core>=5.0.0->pyannote.audio->-r requirements.txt (line 2)) (1.12.0)\n",
      "Requirement already satisfied: pandas>=0.19 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyannote.database>=5.0.1->pyannote.audio->-r requirements.txt (line 2)) (2.2.1)\n",
      "Collecting typer>=0.12.1 (from pyannote.database>=5.0.1->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for typer>=0.12.1 from https://files.pythonhosted.org/packages/20/b5/11cf2e34fbb11b937e006286ab5b8cfd334fde1c8fa4dd7f491226931180/typer-0.12.3-py3-none-any.whl.metadata\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyannote.metrics>=3.2->pyannote.audio->-r requirements.txt (line 2)) (1.4.1.post1)\n",
      "Collecting docopt>=0.6.2 (from pyannote.metrics>=3.2->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Using cached docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tabulate>=0.7.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyannote.metrics>=3.2->pyannote.audio->-r requirements.txt (line 2)) (0.9.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyannote.metrics>=3.2->pyannote.audio->-r requirements.txt (line 2)) (3.8.3)\n",
      "Requirement already satisfied: sympy>=1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyannote.metrics>=3.2->pyannote.audio->-r requirements.txt (line 2)) (1.12)\n",
      "Collecting optuna>=3.1 (from pyannote.pipeline>=3.0.1->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for optuna>=3.1 from https://files.pythonhosted.org/packages/15/da/68883911855d8b4d521f9a370e4e6aab8232b91c1d8d5a8348c4680c6642/optuna-3.6.1-py3-none-any.whl.metadata\n",
      "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from rich>=12.0.0->pyannote.audio->-r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from rich>=12.0.0->pyannote.audio->-r requirements.txt (line 2)) (2.17.2)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/ubuntu/anaconda3/lib/python3.10/site-packages (from soundfile>=0.12.1->pyannote.audio->-r requirements.txt (line 2)) (1.15.1)\n",
      "Collecting hyperpyyaml (from speechbrain>=0.5.14->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for hyperpyyaml from https://files.pythonhosted.org/packages/33/c9/751b6401887f4b50f9307cc1e53d287b3dc77c375c126aeb6335aff73ccb/HyperPyYAML-1.2.2-py3-none-any.whl.metadata\n",
      "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/.local/lib/python3.10/site-packages (from speechbrain>=0.5.14->pyannote.audio->-r requirements.txt (line 2)) (1.3.2)\n",
      "Collecting sentencepiece (from speechbrain>=0.5.14->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for sentencepiece from https://files.pythonhosted.org/packages/a6/27/33019685023221ca8ed98e8ceb7ae5e166032686fa3662c68f1f1edf334e/sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: protobuf>=3.20 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorboardX>=2.6->pyannote.audio->-r requirements.txt (line 2)) (4.25.3)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 1)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 1)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 1)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 1)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 1)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 1)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 1)) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117->-r requirements.txt (line 1)) (12.4.99)\n",
      "Collecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.11.0->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting librosa>=0.6.0 (from torch-audiomentations>=0.11.0->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for librosa>=0.6.0 from https://files.pythonhosted.org/packages/e2/a2/4f639c1168d7aada749a896afb4892a831e2041bebdcf636aebfe9e86556/librosa-0.10.1-py3-none-any.whl.metadata\n",
      "  Downloading librosa-0.10.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.11.0->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for torch-pitch-shift>=1.2.2 from https://files.pythonhosted.org/packages/e6/b3/42b46bccba56baecea9678d8feea09f21777f67ab397719409e036f03058/torch_pitch_shift-1.2.4-py3-none-any.whl.metadata\n",
      "  Downloading torch_pitch_shift-1.2.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from importlib-metadata->speechbox->-r requirements.txt (line 4)) (3.18.0)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /home/ubuntu/.local/lib/python3.10/site-packages (from numba->openai-whisper==20231117->-r requirements.txt (line 1)) (0.42.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ubuntu/.local/lib/python3.10/site-packages (from tiktoken->openai-whisper==20231117->-r requirements.txt (line 1)) (2023.12.25)\n",
      "Requirement already satisfied: pycparser in /home/ubuntu/anaconda3/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio->-r requirements.txt (line 2)) (2.21)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ubuntu/.local/lib/python3.10/site-packages (from fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio->-r requirements.txt (line 2)) (3.9.3)\n",
      "Collecting audioread>=2.1.9 (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for audioread>=2.1.9 from https://files.pythonhosted.org/packages/57/8d/30aa32745af16af0a9a650115fbe81bde7c610ed5c21b381fca0196f3a7f/audioread-3.0.1-py3-none-any.whl.metadata\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->-r requirements.txt (line 2)) (5.1.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /home/ubuntu/anaconda3/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->-r requirements.txt (line 2)) (1.4.0)\n",
      "Collecting soxr>=0.3.2 (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for soxr>=0.3.2 from https://files.pythonhosted.org/packages/31/f7/d95b816c47dca6a068305fb7176b8c8d2c94bbc6cce6dcc296c6cf98660f/soxr-0.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading soxr-0.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting lazy-loader>=0.1 (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for lazy-loader>=0.1 from https://files.pythonhosted.org/packages/83/60/d497a310bde3f01cb805196ac61b7ad6dc5dcf8dce66634dc34364b20b4f/lazy_loader-0.4-py3-none-any.whl.metadata\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/ubuntu/anaconda3/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->-r requirements.txt (line 2)) (1.0.3)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/anaconda3/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning>=2.0.1->pyannote.audio->-r requirements.txt (line 2)) (65.6.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio->-r requirements.txt (line 2)) (0.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio->-r requirements.txt (line 2)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio->-r requirements.txt (line 2)) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio->-r requirements.txt (line 2)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio->-r requirements.txt (line 2)) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio->-r requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Collecting alembic>=1.5.0 (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for alembic>=1.5.0 from https://files.pythonhosted.org/packages/7f/50/9fb3a5c80df6eb6516693270621676980acd6d5a9a7efdbfa273f8d616c7/alembic-1.13.1-py3-none-any.whl.metadata\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for colorlog from https://files.pythonhosted.org/packages/f3/18/3e867ab37a24fdf073c1617b9c7830e06ec270b1ea4694a624038fc40a03/colorlog-6.8.2-py3-none-any.whl.metadata\n",
      "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio->-r requirements.txt (line 2)) (2.0.28)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio->-r requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio->-r requirements.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio->-r requirements.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy>=1.1->pyannote.metrics>=3.2->pyannote.audio->-r requirements.txt (line 2)) (1.3.0)\n",
      "Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for primePy>=1.3 from https://files.pythonhosted.org/packages/74/c1/bb7e334135859c3a92ec399bc89293ea73f28e815e35b43929c8db6af030/primePy-1.3-py3-none-any.whl.metadata\n",
      "  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio->-r requirements.txt (line 2)) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for shellingham>=1.3.0 from https://files.pythonhosted.org/packages/e0/f9/0595336914c5619e5f28a1fb793285925a8cd4b432c9da0a987836c7f822/shellingham-1.5.4-py2.py3-none-any.whl.metadata\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain>=0.5.14->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for ruamel.yaml>=0.17.28 from https://files.pythonhosted.org/packages/73/67/8ece580cc363331d9a53055130f86b096bf16e38156e33b1d3014fffda6b/ruamel.yaml-0.18.6-py3-none-any.whl.metadata\n",
      "  Downloading ruamel.yaml-0.18.6-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from jinja2->torch->openai-whisper==20231117->-r requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio->-r requirements.txt (line 2)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio->-r requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio->-r requirements.txt (line 2)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio->-r requirements.txt (line 2)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio->-r requirements.txt (line 2)) (4.0.3)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for Mako from https://files.pythonhosted.org/packages/c6/c9/9cd84cbd5816aa8bee5fd5a00f857efd636ec30586848d571b67baf0b868/Mako-1.3.3-py3-none-any.whl.metadata\n",
      "  Downloading Mako-1.3.3-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: appdirs in /home/ubuntu/anaconda3/lib/python3.10/site-packages (from pooch>=1.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->-r requirements.txt (line 2)) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio->-r requirements.txt (line 2)) (1.16.0)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=0.5.14->pyannote.audio->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for ruamel.yaml.clib>=0.2.7 from https://files.pythonhosted.org/packages/d3/62/c60b034d9a008bbd566eeecf53a5a4c73d191c8de261290db6761802b72d/ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata\n",
      "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio->-r requirements.txt (line 2)) (3.0.3)\n",
      "Downloading pyannote.audio-3.1.1-py2.py3-none-any.whl (208 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.7/208.7 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading speechbox-0.2.1-py3-none-any.whl (20 kB)\n",
      "Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n",
      "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning-2.2.2-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.database-5.1.0-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n",
      "Downloading pytorch_metric_learning-2.5.0-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.1/119.1 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_17_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading speechbrain-1.0.0-py3-none-any.whl (760 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.1/760.1 kB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch_audiomentations-0.11.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.2.2-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m124.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading more_itertools-10.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading librosa-0.10.1-py3-none-any.whl (253 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.7/253.7 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
      "Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch_pitch_shift-1.2.4-py3-none-any.whl (4.9 kB)\n",
      "Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
      "Downloading pytorch_lightning-2.2.2-py3-none-any.whl (801 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.9/801.9 kB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
      "Downloading ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading soxr-0.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Mako-1.3.3-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: openai-whisper, antlr4-python3-runtime, docopt, julius\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802826 sha256=430733147a53ab1c87b2115992d5dbb9d9a4284fbb7f4b75fb30a7334a3c5a4f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-rho4t9hw/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=c04e57f90306c0eb517ecdd95f2edb2ede5ef5de8bc2b3e129f281e28c1639da\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=b87e6f2f2e8c1ab0489dc13f44e5709a653b3f4ddad04f2b6edb586d689e4e48\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
      "  Building wheel for julius (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21879 sha256=4183d9a02a46485a707b726cab43f14c205471e12ea0a6eef12b9a9a5f38ce9c\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/b9/b2/05/f883527ffcb7f2ead5438a2c23439aa0c881eaa9a4c80256f4\n",
      "Successfully built openai-whisper antlr4-python3-runtime docopt julius\n",
      "Installing collected packages: sentencepiece, pydub, primePy, docopt, antlr4-python3-runtime, tensorboardX, soxr, shellingham, semver, ruamel.yaml.clib, omegaconf, more-itertools, Mako, lightning-utilities, lazy-loader, einops, colorlog, audioread, soundfile, ruamel.yaml, pyannote.core, alembic, typer, optuna, librosa, hyperpyyaml, torchmetrics, torchaudio, speechbox, pytorch-metric-learning, pyannote.database, openai-whisper, julius, asteroid-filterbanks, torch-pitch-shift, speechbrain, pytorch-lightning, pyannote.pipeline, pyannote.metrics, torch-audiomentations, lightning, pyannote.audio\n",
      "  Attempting uninstall: ruamel.yaml.clib\n",
      "    Found existing installation: ruamel.yaml.clib 0.2.6\n",
      "    Uninstalling ruamel.yaml.clib-0.2.6:\n",
      "      Successfully uninstalled ruamel.yaml.clib-0.2.6\n",
      "  Attempting uninstall: ruamel.yaml\n",
      "    Found existing installation: ruamel.yaml 0.17.21\n",
      "    Uninstalling ruamel.yaml-0.17.21:\n",
      "      Successfully uninstalled ruamel.yaml-0.17.21\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.9.0\n",
      "    Uninstalling typer-0.9.0:\n",
      "      Successfully uninstalled typer-0.9.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
      "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Mako-1.3.3 alembic-1.13.1 antlr4-python3-runtime-4.9.3 asteroid-filterbanks-0.4.0 audioread-3.0.1 colorlog-6.8.2 docopt-0.6.2 einops-0.7.0 hyperpyyaml-1.2.2 julius-0.2.7 lazy-loader-0.4 librosa-0.10.1 lightning-2.2.2 lightning-utilities-0.11.2 more-itertools-10.2.0 omegaconf-2.3.0 openai-whisper-20231117 optuna-3.6.1 primePy-1.3 pyannote.audio-3.1.1 pyannote.core-5.0.0 pyannote.database-5.1.0 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pydub-0.25.1 pytorch-lightning-2.2.2 pytorch-metric-learning-2.5.0 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8 semver-3.0.2 sentencepiece-0.2.0 shellingham-1.5.4 soundfile-0.12.1 soxr-0.3.7 speechbox-0.2.1 speechbrain-1.0.0 tensorboardX-2.6.2.2 torch-audiomentations-0.11.1 torch-pitch-shift-1.2.4 torchaudio-2.2.2 torchmetrics-1.3.2 typer-0.12.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\",device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.10/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyannote.audio.pipelines.speaker_diarization.SpeakerDiarization at 0x7f13beb0b4f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline.from_pretrained(\n",
    "  \"pyannote/speaker-diarization-3.1\",\n",
    "  use_auth_token=\"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\")\n",
    "pipeline.to(torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbox import ASRDiarizationPipeline\n",
    "import torch\n",
    "pipeline1 = ASRDiarizationPipeline(\n",
    "    asr_pipeline=pipe, diarization_pipeline=pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Annotation' object has no attribute 'for_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m t\u001b[38;5;241m=\u001b[39m\u001b[43mpipeline1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/ubuntu/ambuje/speech/take_home_naval.wav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/speechbox/diarize.py:90\u001b[0m, in \u001b[0;36mASRDiarizationPipeline.__call__\u001b[0;34m(self, inputs, group_by_speaker, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m inputs, diarizer_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs)\n\u001b[1;32m     85\u001b[0m diarization \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiarization_pipeline(\n\u001b[1;32m     86\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaveform\u001b[39m\u001b[38;5;124m\"\u001b[39m: diarizer_inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_rate},\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     88\u001b[0m )\n\u001b[0;32m---> 90\u001b[0m segments \u001b[38;5;241m=\u001b[39m \u001b[43mdiarization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_json\u001b[49m()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# diarizer output may contain consecutive segments from the same speaker (e.g. {(0 -> 1, speaker_1), (1 -> 1.5, speaker_1), ...})\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# we combine these segments to give overall timestamps for each speaker's turn (e.g. {(0 -> 1.5, speaker_1), ...})\u001b[39;00m\n\u001b[1;32m     94\u001b[0m new_segments \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Annotation' object has no attribute 'for_json'"
     ]
    }
   ],
   "source": [
    "t=pipeline1('/home/ubuntu/ambuje/speech/take_home_naval.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.10/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "AutomaticSpeechRecognitionPipeline._sanitize_parameters() got an unexpected keyword argument 'use_auth_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mASRDiarizationPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai/whisper-large-v3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/speechbox/diarize.py:33\u001b[0m, in \u001b[0;36mASRDiarizationPipeline.from_pretrained\u001b[0;34m(cls, asr_model, diarizer_model, chunk_length_s, use_auth_token, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     32\u001b[0m ):\n\u001b[0;32m---> 33\u001b[0m     asr_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mautomatic-speech-recognition\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masr_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_length_s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_length_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     diarization_pipeline \u001b[38;5;241m=\u001b[39m Pipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(diarizer_model, use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(asr_pipeline, diarization_pipeline)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/__init__.py:1107\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m-> 1107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:220\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.__init__\u001b[0;34m(self, model, feature_extractor, tokenizer, decoder, device, torch_dtype, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 220\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:886\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 886\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_params, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_params, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_postprocess_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;66;03m# Pipelines calling `generate`: if the tokenizer has a pad token but the model doesn't, set it in the\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;66;03m# forward params so that `generate` is aware of the pad token.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcan_generate()\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    895\u001b[0m ):\n",
      "\u001b[0;31mTypeError\u001b[0m: AutomaticSpeechRecognitionPipeline._sanitize_parameters() got an unexpected keyword argument 'use_auth_token'"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from speechbox import ASRDiarizationPipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = ASRDiarizationPipeline.from_pretrained(\"openai/whisper-large-v3\", device=device,use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spk1', 0.0, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "def split_segments(segments):\n",
    "    split_segments = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        if segment:\n",
    "            speaker, start_time, end_time = segment\n",
    "            if split_segments and split_segments[-1]:\n",
    "                _, prev_start_time, prev_end_time = split_segments[-1]\n",
    "                if prev_end_time < start_time:\n",
    "                    split_segments.append((speaker, start_time, end_time))\n",
    "                else:\n",
    "                    split_segments[-1] = (split_segments[-1][0], split_segments[-1][1], start_time)\n",
    "            else:\n",
    "                split_segments.append(segment)\n",
    "        else:\n",
    "            split_segments.append(segment)\n",
    "    \n",
    "    return split_segments\n",
    "\n",
    "segments = [\n",
    "    ('spk1', 0.0, 1.25),\n",
    "    ('spk2', 1.0, 1.2)\n",
    "]\n",
    "\n",
    "split_segments = split_segments(segments)\n",
    "print(split_segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.10/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.03 GiB of which 14.94 MiB is free. Process 37645 has 18.47 GiB memory in use. Including non-PyTorch memory, this process has 3.54 GiB memory in use. Of the allocated memory 3.23 GiB is allocated by PyTorch, and 77.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mASRDiarizationPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai/whisper-large-v3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# load dataset of concatenated LibriSpeech samples\u001b[39;00m\n\u001b[1;32m      9\u001b[0m concatenated_librispeech \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msanchit-gandhi/concatenated_librispeech\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/speechbox/diarize.py:33\u001b[0m, in \u001b[0;36mASRDiarizationPipeline.from_pretrained\u001b[0;34m(cls, asr_model, diarizer_model, chunk_length_s, use_auth_token, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     32\u001b[0m ):\n\u001b[0;32m---> 33\u001b[0m     asr_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mautomatic-speech-recognition\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masr_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_length_s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_length_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     diarization_pipeline \u001b[38;5;241m=\u001b[39m Pipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(diarizer_model, use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(asr_pipeline, diarization_pipeline)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/__init__.py:1107\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m-> 1107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:220\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.__init__\u001b[0;34m(self, model, feature_extractor, tokenizer, decoder, device, torch_dtype, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 220\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:874\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    873\u001b[0m ):\n\u001b[0;32m--> 874\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# Update config and generation_config with task specific parameters\u001b[39;00m\n\u001b[1;32m    877\u001b[0m task_specific_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtask_specific_params\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:2576\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2572\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2573\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2575\u001b[0m         )\n\u001b[0;32m-> 2576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.03 GiB of which 14.94 MiB is free. Process 37645 has 18.47 GiB memory in use. Including non-PyTorch memory, this process has 3.54 GiB memory in use. Of the allocated memory 3.23 GiB is allocated by PyTorch, and 77.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from speechbox import ASRDiarizationPipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = ASRDiarizationPipeline.from_pretrained(\"openai/whisper-large-v3\", device=device)\n",
    "\n",
    "# load dataset of concatenated LibriSpeech samples\n",
    "concatenated_librispeech = load_dataset(\"sanchit-gandhi/concatenated_librispeech\", split=\"train\", streaming=True)\n",
    "# get first sample\n",
    "sample = next(iter(concatenated_librispeech))\n",
    "\n",
    "out = pipeline(sample[\"audio\"])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d360e3a28d47fea4f0e37c4bad7d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/359 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concatenated_librispeech = load_dataset(\"sanchit-gandhi/concatenated_librispeech\", split=\"train\", streaming=True)\n",
    "# get first sample\n",
    "sample = next(iter(concatenated_librispeech))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'processed.wav',\n",
       " 'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00097656,\n",
       "        -0.00109863, -0.00146484]),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['audio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline.from_pretrained(\n",
    "  \"pyannote/speaker-diarization-3.1\",\n",
    "  use_auth_token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyannote.audio.pipelines.speaker_diarization.SpeakerDiarization at 0x7fd6d3c16440>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "pipeline.to(torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "waveform, sample_rate = torchaudio.load(\"take_home_naval.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3357f76fec4a4a9083b185172a481f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "with ProgressHook() as hook:\n",
    "  diarization = pipeline({\"waveform\": waveform, \"sample_rate\": sample_rate},hook=hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<pyannote.core.annotation.Annotation at 0x7fd49c6e47c0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diarization.itersegments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the speakers and their speaking intervals\n",
    "# intervals = [\n",
    "#     (\"SPEAKER_00\", 0.008, 59.889),\n",
    "#     (\"SPEAKER_01\", 16.646, 17.495),\n",
    "#     (\"SPEAKER_01\", 17.699, 18.208),\n",
    "#     (\"SPEAKER_01\", 35.237, 35.696),\n",
    "#     (\"SPEAKER_01\", 50.042, 50.653),\n",
    "#     (\"SPEAKER_00\", 60.415, 147.376)\n",
    "# ]\n",
    "def split_overlap(intervals):\n",
    "    r=[]\n",
    "# Sort the intervals by start time\n",
    "    intervals.sort(key=lambda x: x[1])\n",
    "\n",
    "    # Initialize the result\n",
    "    result = []\n",
    "\n",
    "    # Iterate over the intervals\n",
    "    for speaker, start, end in intervals:\n",
    "        # If the result is not empty and the current interval overlaps with the last interval in the result\n",
    "        if result and start < result[-1][2]:\n",
    "            # Split the last interval in the result\n",
    "            last_speaker, last_start, last_end = result.pop()\n",
    "            result.append((last_speaker, last_start, start))\n",
    "            result.append((speaker, start, min(end, last_end)))\n",
    "            if end < last_end:\n",
    "                result.append((last_speaker, end, last_end))\n",
    "        else:\n",
    "            # Add the current interval to the result\n",
    "            result.append((speaker, start, end))\n",
    "\n",
    "    # Print the result\n",
    "    for speaker, start, end in result:\n",
    "        r.append((speaker,start,end))\n",
    "        # print(f\"{speaker} {start}--{end}\")\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"diarization.txt\", \"w\") as text_file:\n",
    "    text_file.write(str(diarization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def millisec(timeStr):\n",
    "  spl = timeStr.split(\":\")\n",
    "  s = (int)((int(spl[0]) * 60 * 60 + int(spl[1]) * 60 + float(spl[2]) )* 1000)\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "dzs = open('diarization.txt').read().splitlines()\n",
    "\n",
    "groups = []\n",
    "g = []\n",
    "lastend = 0\n",
    "\n",
    "for d in dzs:   \n",
    "  if g and (g[0].split()[-1] != d.split()[-1]):      #same speaker\n",
    "    groups.append(g)\n",
    "    g = []\n",
    "  \n",
    "  g.append(d)\n",
    "  \n",
    "  end = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=d)[1]\n",
    "  end = millisec(end)\n",
    "  if (lastend > end):       #segment engulfed by a previous segment\n",
    "    groups.append(g)\n",
    "    g = [] \n",
    "  else:\n",
    "    lastend = end\n",
    "if g:\n",
    "  groups.append(g)\n",
    "# print(*groups, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the speakers and their speaking intervals\n",
    "# intervals = [\n",
    "#     (\"SPEAKER_00\", 0.008, 59.889),\n",
    "#     (\"SPEAKER_01\", 16.646, 17.495),\n",
    "#     (\"SPEAKER_01\", 17.699, 18.208),\n",
    "#     (\"SPEAKER_01\", 35.237, 35.696),\n",
    "#     (\"SPEAKER_01\", 50.042, 50.653),\n",
    "#     (\"SPEAKER_00\", 60.415, 287.665)\n",
    "# ]\n",
    "def combine_timestamp(intervals):\n",
    "    r=[]\n",
    "# Sort the intervals by start time\n",
    "    intervals.sort(key=lambda x: x[1])\n",
    "\n",
    "    # Initialize the result\n",
    "    result = []\n",
    "\n",
    "    # Iterate over the intervals\n",
    "    for speaker, start, end in intervals:\n",
    "        # If the result is not empty and the current interval overlaps with the last interval in the result\n",
    "        if result and start - result[-1][2] < 1000 and speaker == result[-1][0]:\n",
    "            # Merge the current interval with the last interval in the result\n",
    "            last_speaker, last_start, last_end = result.pop()\n",
    "            result.append((last_speaker, last_start, max(end, last_end)))\n",
    "        else:\n",
    "            # Add the current interval to the result\n",
    "            result.append((speaker, start, end))\n",
    "\n",
    "    # Print the result\n",
    "    for speaker, start, end in result:\n",
    "        r.append((speaker,start,end))\n",
    "        # print(f\"{speaker} {start}--{end}\")\n",
    "    return r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_info=[]\n",
    "for g in groups:\n",
    "  start = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[0])[0]\n",
    "  end = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[-1])[1]\n",
    "  start = millisec(start) #- spacermilli\n",
    "  end = millisec(end)\n",
    "  speaker = g[0].split()[-1]\n",
    "  asr_info.append((str(speaker),start,end))\n",
    "  combine_time_stamp_asr=combine_timestamp(asr_info)\n",
    "  asr_overlap_list=split_overlap(combine_time_stamp_asr)\n",
    "  # print(start,end,speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_time_stamp_asr=combine_timestamp(asr_info)\n",
    "asr_overlap_list=split_overlap(combine_time_stamp_asr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(asr_overlap_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SPEAKER_00', 8, 16646),\n",
       " ('SPEAKER_01', 16646, 18208),\n",
       " ('SPEAKER_00', 18208, 35237),\n",
       " ('SPEAKER_01', 35237, 35696),\n",
       " ('SPEAKER_00', 35696, 50042),\n",
       " ('SPEAKER_01', 50042, 50653),\n",
       " ('SPEAKER_00', 50653, 59889),\n",
       " ('SPEAKER_00', 60415, 243370),\n",
       " ('SPEAKER_01', 243370, 243743),\n",
       " ('SPEAKER_00', 243743, 254049),\n",
       " ('SPEAKER_01', 254049, 254575),\n",
       " ('SPEAKER_00', 254575, 273115),\n",
       " ('SPEAKER_01', 273115, 273573),\n",
       " ('SPEAKER_00', 273573, 287665),\n",
       " ('SPEAKER_00', 288972, 484100),\n",
       " ('SPEAKER_01', 484100, 484643),\n",
       " ('SPEAKER_00', 484643, 489685),\n",
       " ('SPEAKER_00', 489753, 496052),\n",
       " ('SPEAKER_01', 496052, 497020),\n",
       " ('SPEAKER_00', 497020, 502724),\n",
       " ('SPEAKER_00', 503302, 536697),\n",
       " ('SPEAKER_01', 536697, 537427),\n",
       " ('SPEAKER_00', 537427, 540348),\n",
       " ('SPEAKER_00', 540704, 546086),\n",
       " ('SPEAKER_01', 546086, 546935),\n",
       " ('SPEAKER_00', 546935, 547920),\n",
       " ('SPEAKER_00', 548820, 846018),\n",
       " ('SPEAKER_01', 846120, 987835)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_overlap_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = AudioSegment.from_wav(\"take_home_naval.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pydub.audio_segment.AudioSegment object at 0x7fd5013ac880>\n"
     ]
    }
   ],
   "source": [
    "for i in asr_overlap_list:\n",
    "    print(audio[i[1]:i[2]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asr_extraction import diarization_asr_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b14d057606f482d8904d36caa73d325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/ubuntu/anaconda3/lib/python3.10/site-packages/pyannote/audio/utils/reproducibility.py:74: \n",
       "ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and \n",
       "lower accuracy.\n",
       "It can be re-enabled by calling\n",
       "   &gt;&gt;&gt; import torch\n",
       "   &gt;&gt;&gt; torch.backends.cuda.matmul.allow_tf32 = True\n",
       "   &gt;&gt;&gt; torch.backends.cudnn.allow_tf32 = True\n",
       "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
       "\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/ubuntu/anaconda3/lib/python3.10/site-packages/pyannote/audio/utils/reproducibility.py:74: \n",
       "ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and \n",
       "lower accuracy.\n",
       "It can be re-enabled by calling\n",
       "   >>> import torch\n",
       "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
       "   >>> torch.backends.cudnn.allow_tf32 = True\n",
       "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
       "\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/ubuntu/anaconda3/lib/python3.10/site-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: \n",
       "std(): degrees of freedom is &lt;= 0. Correction should be strictly less than the reduction factor (input numel \n",
       "divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1760.)\n",
       "  std = sequences.std(dim=-1, correction=1)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/ubuntu/anaconda3/lib/python3.10/site-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: \n",
       "std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel \n",
       "divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1760.)\n",
       "  std = sequences.std(dim=-1, correction=1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diarization file saved\n",
      "Audio file processed  1 / 28\n",
      "Audio file processed  2 / 28\n",
      "Audio file processed  3 / 28\n",
      "Audio file processed  4 / 28\n",
      "Audio file processed  5 / 28\n",
      "Audio file processed  6 / 28\n",
      "Audio file processed  7 / 28\n",
      "Audio file processed  8 / 28\n",
      "Audio file processed  9 / 28\n",
      "Audio file processed  10 / 28\n",
      "Audio file processed  11 / 28\n",
      "Audio file processed  12 / 28\n",
      "Audio file processed  13 / 28\n",
      "Audio file processed  14 / 28\n",
      "Audio file processed  15 / 28\n",
      "Audio file processed  16 / 28\n",
      "Audio file processed  17 / 28\n",
      "Audio file processed  18 / 28\n",
      "Audio file processed  19 / 28\n",
      "Audio file processed  20 / 28\n",
      "Audio file processed  21 / 28\n",
      "Audio file processed  22 / 28\n",
      "Audio file processed  23 / 28\n",
      "Audio file processed  24 / 28\n",
      "Audio file processed  25 / 28\n",
      "Audio file processed  26 / 28\n",
      "Audio file processed  27 / 28\n",
      "Audio file processed  28 / 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel Information</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>If you find a mountain and you start climbing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>People don't want to start over.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>And it's the nature of later in life that you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Mm-hmm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>called an idiot and start over doing something...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>times they'll just miss completely. So you hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Nivi just made the point to me on the side tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>You don't want to spendit waiting in line. You...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>And thenyou get to be you. And no one in the w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>But if you are a strong person, if astudent li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>and it's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>If they want to compete with me and they're go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>So it's your unlimited desires that are cloudi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>That's what it means.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>right? Every desire you have is an access wher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>The reality is I don't actually read that much...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>I would argue the important thing is to read e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>And then you start eating. You start eating th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>But knowing arithmetic really well will help y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Mm-hmm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>This is why there are a lot of people, I'm sur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>And you can't quite follow their reasoning. Yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>So if you look at the most powerful thinkers, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>They have to understand the basics really, rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Mmm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>should be able to re-derive anything on the spot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>And if you can't, you don't know it. So do you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Everything.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Everything. Everything.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>The first thing, if you're going to make money...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>They're going to pay you thebare minimum that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>So the first thing you have todo is you have t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>For capital leverage, somebody has to give you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>To me, the real winners are the ones who step ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>And if you couldliterally just sit, if you cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Now, let me leave you with some words from Ste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>The second is that you are not aprofessional. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>Personally, my main three sources of learning ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Channel Information                                               text\n",
       "0           SPEAKER_00  If you find a mountain and you start climbing ...\n",
       "1           SPEAKER_01                   People don't want to start over.\n",
       "2           SPEAKER_00  And it's the nature of later in life that you ...\n",
       "3           SPEAKER_01                                            Mm-hmm.\n",
       "4           SPEAKER_00  called an idiot and start over doing something...\n",
       "5           SPEAKER_01                                                No.\n",
       "6           SPEAKER_00  times they'll just miss completely. So you hav...\n",
       "7           SPEAKER_00  Nivi just made the point to me on the side tha...\n",
       "8           SPEAKER_00  You don't want to spendit waiting in line. You...\n",
       "9           SPEAKER_00  And thenyou get to be you. And no one in the w...\n",
       "10          SPEAKER_00  But if you are a strong person, if astudent li...\n",
       "11          SPEAKER_01                                           and it's\n",
       "12          SPEAKER_00  If they want to compete with me and they're go...\n",
       "13          SPEAKER_01                                             Right.\n",
       "14          SPEAKER_00  So it's your unlimited desires that are cloudi...\n",
       "15          SPEAKER_01                              That's what it means.\n",
       "16          SPEAKER_00  right? Every desire you have is an access wher...\n",
       "17          SPEAKER_00  The reality is I don't actually read that much...\n",
       "18          SPEAKER_00  I would argue the important thing is to read e...\n",
       "19          SPEAKER_00  And then you start eating. You start eating th...\n",
       "20          SPEAKER_00  But knowing arithmetic really well will help y...\n",
       "21          SPEAKER_01                                            Mm-hmm.\n",
       "22          SPEAKER_00  This is why there are a lot of people, I'm sur...\n",
       "23          SPEAKER_00  And you can't quite follow their reasoning. Yo...\n",
       "24          SPEAKER_01                                             Right.\n",
       "25          SPEAKER_00  So if you look at the most powerful thinkers, ...\n",
       "26          SPEAKER_00  They have to understand the basics really, rea...\n",
       "27          SPEAKER_01                                               Mmm.\n",
       "28          SPEAKER_00  should be able to re-derive anything on the spot.\n",
       "29          SPEAKER_00  And if you can't, you don't know it. So do you...\n",
       "30          SPEAKER_01                                        Everything.\n",
       "31          SPEAKER_00                            Everything. Everything.\n",
       "32          SPEAKER_00  The first thing, if you're going to make money...\n",
       "33          SPEAKER_00  They're going to pay you thebare minimum that ...\n",
       "34          SPEAKER_00  So the first thing you have todo is you have t...\n",
       "35          SPEAKER_00  For capital leverage, somebody has to give you...\n",
       "36          SPEAKER_00  To me, the real winners are the ones who step ...\n",
       "37          SPEAKER_00  And if you couldliterally just sit, if you cou...\n",
       "38          SPEAKER_01  Now, let me leave you with some words from Ste...\n",
       "39          SPEAKER_01  The second is that you are not aprofessional. ...\n",
       "40          SPEAKER_01  Personally, my main three sources of learning ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diarization_asr_output('/home/ubuntu/ambuje/speech/take_home_naval.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "from utils import group_,combine_timestamp,split_overlap,millisec,break_string_near_k_words\n",
    "import re\n",
    "import whisper\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "from utils import group_,combine_timestamp,split_overlap,millisec,break_string_near_k_words\n",
    "import re\n",
    "import whisper\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def final_speaker_end_start(groups):\n",
    "    asr_info=[]\n",
    "    for g in groups:\n",
    "        start = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[0])[0]\n",
    "        end = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[-1])[1]\n",
    "        start = millisec(start) #- spacermilli\n",
    "        end = millisec(end)\n",
    "        speaker = g[0].split()[-1]\n",
    "        asr_info.append((str(speaker),start,end))\n",
    "    combine_time_stamp_asr=combine_timestamp(asr_info)\n",
    "    asr_overlap_list=split_overlap(combine_time_stamp_asr)\n",
    "    return asr_overlap_list\n",
    "\n",
    "\n",
    "    pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\",use_auth_token=\"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\")\n",
    "    pipeline.to(torch.device(\"cuda\"))\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    with ProgressHook() as hook:\n",
    "        diarization = pipeline({\"waveform\": waveform, \"sample_rate\": sample_rate},hook=hook)\n",
    "    with open(\"diarization.txt\", \"w\") as text_file:\n",
    "        text_file.write(str(diarization))\n",
    "    print('Diarization file saved')\n",
    "    groups=group_()\n",
    "    asr_overlap_list=final_speaker_end_start(groups)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = whisper.load_model('large', device = device)\n",
    "    r_t=[]\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    speak=[]\n",
    "    cnt=1\n",
    "    for i in asr_overlap_list:\n",
    "        result = model.transcribe(np.frombuffer(audio[i[1]:i[2]].raw_data, np.int16).flatten().astype(np.float32) / 32768.0, language='en', word_timestamps=True)\n",
    "        r_t.append(result['text'])\n",
    "        speak.append(i[0])\n",
    "        print('Audio file processed ',cnt,len(asr_overlap_list))\n",
    "        cnt=cnt+1\n",
    "    text=[]\n",
    "    f_speaker=[]\n",
    "    for i in range(0,len(r_t)):\n",
    "        tmp=break_string_near_k_words(r_t[i],200)\n",
    "        f_speaker.extend([speak[i]]*len(tmp))\n",
    "        text.extend(tmp)\n",
    "    df=pd.DataFrame({'Channel Information':f_speaker,'text':text})\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_speaker_end_start(groups):\n",
    "    asr_info=[]\n",
    "    for g in groups:\n",
    "        start = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[0])[0]\n",
    "        end = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[-1])[1]\n",
    "        start = millisec(start) #- spacermilli\n",
    "        end = millisec(end)\n",
    "        speaker = g[0].split()[-1]\n",
    "        asr_info.append((str(speaker),start,end))\n",
    "    combine_time_stamp_asr=combine_timestamp(asr_info)\n",
    "    asr_overlap_list=split_overlap(combine_time_stamp_asr)\n",
    "    return asr_overlap_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e158a7e916a463cb455b6a5ff9c5fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diarization file saved\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'final_speaker_end_start' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiarization file saved\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m groups\u001b[38;5;241m=\u001b[39mgroup_()\n\u001b[0;32m---> 11\u001b[0m asr_overlap_list\u001b[38;5;241m=\u001b[39m\u001b[43mfinal_speaker_end_start\u001b[49m(groups)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_speaker_end_start' is not defined"
     ]
    }
   ],
   "source": [
    "audio_path='/home/ubuntu/ambuje/speech/take_home_naval.wav'\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\",use_auth_token=\"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\")\n",
    "pipeline.to(torch.device(\"cuda\"))\n",
    "waveform, sample_rate = torchaudio.load(audio_path)\n",
    "with ProgressHook() as hook:\n",
    "    diarization = pipeline({\"waveform\": waveform, \"sample_rate\": sample_rate},hook=hook)\n",
    "with open(\"diarization.txt\", \"w\") as text_file:\n",
    "    text_file.write(str(diarization))\n",
    "print('Diarization file saved')\n",
    "groups=group_()\n",
    "asr_overlap_list=final_speaker_end_start(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups=group_()\n",
    "asr_overlap_list=final_speaker_end_start(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SPEAKER_01', 846120, 987835)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_overlap_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def groups():\n",
    "  dzs = open('diarization.txt').read().splitlines()\n",
    "\n",
    "  groups = []\n",
    "  g = []\n",
    "  lastend = 0\n",
    "\n",
    "  for d in dzs:   \n",
    "    if g and (g[0].split()[-1] != d.split()[-1]):      #same speaker\n",
    "      groups.append(g)\n",
    "      g = []\n",
    "    \n",
    "    g.append(d)\n",
    "    \n",
    "    end = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=d)[1]\n",
    "    end = millisec(end)\n",
    "    if (lastend > end):       #segment engulfed by a previous segment\n",
    "      groups.append(g)\n",
    "      g = [] \n",
    "    else:\n",
    "      lastend = end\n",
    "  if g:\n",
    "    groups.append(g)\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1=groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_info=[]\n",
    "for g in g1:\n",
    "    start = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[0])[0]\n",
    "    end = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[-1])[1]\n",
    "    start = millisec(start) #- spacermilli\n",
    "    end = millisec(end)\n",
    "    speaker = g[0].split()[-1]\n",
    "    asr_info.append((str(speaker),start,end))\n",
    "combine_time_stamp_asr=combine_timestamp(asr_info)\n",
    "asr_overlap_list=split_overlap(combine_time_stamp_asr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SPEAKER_00', 8, 16646),\n",
       " ('SPEAKER_01', 16646, 18208),\n",
       " ('SPEAKER_00', 18208, 35237),\n",
       " ('SPEAKER_01', 35237, 35696),\n",
       " ('SPEAKER_00', 35696, 50042),\n",
       " ('SPEAKER_01', 50042, 50653),\n",
       " ('SPEAKER_00', 50653, 59889),\n",
       " ('SPEAKER_00', 60415, 243370),\n",
       " ('SPEAKER_01', 243370, 243743),\n",
       " ('SPEAKER_00', 243743, 254049),\n",
       " ('SPEAKER_01', 254049, 254575),\n",
       " ('SPEAKER_00', 254575, 273115),\n",
       " ('SPEAKER_01', 273115, 273573),\n",
       " ('SPEAKER_00', 273573, 287665),\n",
       " ('SPEAKER_00', 288972, 484100),\n",
       " ('SPEAKER_01', 484100, 484643),\n",
       " ('SPEAKER_00', 484643, 489685),\n",
       " ('SPEAKER_00', 489753, 496052),\n",
       " ('SPEAKER_01', 496052, 497020),\n",
       " ('SPEAKER_00', 497020, 502724),\n",
       " ('SPEAKER_00', 503302, 536697),\n",
       " ('SPEAKER_01', 536697, 537427),\n",
       " ('SPEAKER_00', 537427, 540348),\n",
       " ('SPEAKER_00', 540704, 546086),\n",
       " ('SPEAKER_01', 546086, 546935),\n",
       " ('SPEAKER_00', 546935, 547920),\n",
       " ('SPEAKER_00', 548820, 846018),\n",
       " ('SPEAKER_01', 846120, 987835)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_overlap_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SPEAKER_00', '00:00:00.008', '00:00:59.889'),\n",
       " ('SPEAKER_01', '00:00:16.646', '00:00:17.495'),\n",
       " ('SPEAKER_01', '00:00:17.699', '00:00:18.208'),\n",
       " ('SPEAKER_01', '00:00:35.237', '00:00:35.696'),\n",
       " ('SPEAKER_01', '00:00:50.042', '00:00:50.653'),\n",
       " ('SPEAKER_00', '00:01:00.415', '00:04:47.665'),\n",
       " ('SPEAKER_01', '00:04:03.370', '00:04:03.743'),\n",
       " ('SPEAKER_01', '00:04:14.049', '00:04:14.575'),\n",
       " ('SPEAKER_01', '00:04:33.115', '00:04:33.573'),\n",
       " ('SPEAKER_00', '00:04:48.972', '00:08:09.685'),\n",
       " ('SPEAKER_01', '00:08:04.100', '00:08:04.643'),\n",
       " ('SPEAKER_00', '00:08:09.753', '00:08:22.724'),\n",
       " ('SPEAKER_01', '00:08:16.052', '00:08:17.020'),\n",
       " ('SPEAKER_00', '00:08:23.302', '00:09:00.348'),\n",
       " ('SPEAKER_01', '00:08:56.697', '00:08:57.427'),\n",
       " ('SPEAKER_00', '00:09:00.704', '00:09:07.920'),\n",
       " ('SPEAKER_01', '00:09:06.086', '00:09:06.935'),\n",
       " ('SPEAKER_00', '00:09:08.820', '00:14:06.018'),\n",
       " ('SPEAKER_01', '00:14:06.120', '00:16:27.835')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m turn, _,speaker \u001b[38;5;129;01min\u001b[39;00m diarization\u001b[38;5;241m.\u001b[39mitersegments():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(turn,_)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "for turn, _, speaker in diarization.itersegments():\n",
    "    print(turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = {}\n",
    "speaker_tags = []\n",
    "common = []\n",
    "\n",
    "    # create a dictionary of SPEAKER_XX to real name mappings\n",
    "speaker_map = {}\n",
    "\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "\n",
    "    start = round(turn.start, 1)\n",
    "    end = round(turn.end, 1)\n",
    "    common.append([start, end, speaker])\n",
    "\n",
    "    # find different speakers\n",
    "    if speaker not in speaker_tags:\n",
    "        speaker_tags.append(speaker)\n",
    "        speaker_map[speaker] = speaker\n",
    "        speakers[speaker] = []\n",
    "\n",
    "    speakers[speaker].append([start, end, speaker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SPEAKER_00': [[0.0, 59.9, 'SPEAKER_00'],\n",
       "  [60.4, 147.4, 'SPEAKER_00'],\n",
       "  [147.8, 171.4, 'SPEAKER_00'],\n",
       "  [171.8, 201.4, 'SPEAKER_00'],\n",
       "  [201.9, 238.7, 'SPEAKER_00'],\n",
       "  [238.9, 287.7, 'SPEAKER_00'],\n",
       "  [289.0, 297.6, 'SPEAKER_00'],\n",
       "  [298.2, 310.3, 'SPEAKER_00'],\n",
       "  [310.8, 361.6, 'SPEAKER_00'],\n",
       "  [361.9, 372.7, 'SPEAKER_00'],\n",
       "  [373.1, 417.1, 'SPEAKER_00'],\n",
       "  [417.3, 421.4, 'SPEAKER_00'],\n",
       "  [422.0, 431.4, 'SPEAKER_00'],\n",
       "  [432.0, 440.1, 'SPEAKER_00'],\n",
       "  [440.3, 441.2, 'SPEAKER_00'],\n",
       "  [441.4, 489.7, 'SPEAKER_00'],\n",
       "  [489.8, 502.7, 'SPEAKER_00'],\n",
       "  [503.3, 540.3, 'SPEAKER_00'],\n",
       "  [540.7, 542.1, 'SPEAKER_00'],\n",
       "  [542.8, 547.9, 'SPEAKER_00'],\n",
       "  [548.8, 553.0, 'SPEAKER_00'],\n",
       "  [553.3, 628.9, 'SPEAKER_00'],\n",
       "  [629.0, 643.8, 'SPEAKER_00'],\n",
       "  [644.1, 653.5, 'SPEAKER_00'],\n",
       "  [653.9, 665.3, 'SPEAKER_00'],\n",
       "  [666.4, 730.1, 'SPEAKER_00'],\n",
       "  [730.8, 767.5, 'SPEAKER_00'],\n",
       "  [768.3, 772.0, 'SPEAKER_00'],\n",
       "  [772.4, 774.7, 'SPEAKER_00'],\n",
       "  [775.1, 777.7, 'SPEAKER_00'],\n",
       "  [778.2, 782.4, 'SPEAKER_00'],\n",
       "  [782.9, 785.7, 'SPEAKER_00'],\n",
       "  [786.1, 786.7, 'SPEAKER_00'],\n",
       "  [786.8, 791.9, 'SPEAKER_00'],\n",
       "  [792.3, 795.3, 'SPEAKER_00'],\n",
       "  [795.5, 795.8, 'SPEAKER_00'],\n",
       "  [795.9, 798.3, 'SPEAKER_00'],\n",
       "  [798.7, 803.1, 'SPEAKER_00'],\n",
       "  [803.5, 806.5, 'SPEAKER_00'],\n",
       "  [806.7, 809.0, 'SPEAKER_00'],\n",
       "  [809.4, 816.0, 'SPEAKER_00'],\n",
       "  [816.4, 821.0, 'SPEAKER_00'],\n",
       "  [821.6, 823.7, 'SPEAKER_00'],\n",
       "  [824.3, 826.2, 'SPEAKER_00'],\n",
       "  [826.5, 828.4, 'SPEAKER_00'],\n",
       "  [828.7, 832.1, 'SPEAKER_00'],\n",
       "  [833.5, 834.0, 'SPEAKER_00'],\n",
       "  [834.2, 846.0, 'SPEAKER_00']],\n",
       " 'SPEAKER_01': [[16.6, 17.5, 'SPEAKER_01'],\n",
       "  [17.7, 18.2, 'SPEAKER_01'],\n",
       "  [35.2, 35.7, 'SPEAKER_01'],\n",
       "  [50.0, 50.7, 'SPEAKER_01'],\n",
       "  [243.4, 243.7, 'SPEAKER_01'],\n",
       "  [254.0, 254.6, 'SPEAKER_01'],\n",
       "  [273.1, 273.6, 'SPEAKER_01'],\n",
       "  [484.1, 484.6, 'SPEAKER_01'],\n",
       "  [496.1, 497.0, 'SPEAKER_01'],\n",
       "  [536.7, 537.4, 'SPEAKER_01'],\n",
       "  [546.1, 546.9, 'SPEAKER_01'],\n",
       "  [846.1, 987.8, 'SPEAKER_01']]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_remove = []\n",
    "merged = []\n",
    "\n",
    "# merging same speakers\n",
    "for spk_tag1, spk_segments1 in speakers.items():\n",
    "    for spk_tag2, spk_segments2 in speakers.items():\n",
    "        if spk_tag1 not in merged and spk_tag2 not in merged and spk_tag1 != spk_tag2 and speaker_map[spk_tag1] == speaker_map[spk_tag2]:\n",
    "            for segment in spk_segments2:\n",
    "                speakers[spk_tag1].append(segment)\n",
    "\n",
    "            merged.append(spk_tag1)\n",
    "            merged.append(spk_tag2)\n",
    "            keys_to_remove.append(spk_tag2)\n",
    "\n",
    "# fixing the speaker names in common\n",
    "for segment in common:\n",
    "    speaker = segment[2]\n",
    "    segment[2] = speaker_map[speaker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_segments = []\n",
    "\n",
    "for item in common:\n",
    "    speaker = item[2]\n",
    "    start = item[0]\n",
    "    end = item[1]\n",
    "\n",
    "    for spk_tag, spk_segments in speakers.items():\n",
    "        if speaker == speaker_map[spk_tag]:\n",
    "            for segment in spk_segments:\n",
    "                if start == segment[0] and end == segment[1]:\n",
    "                    common_segments.append([start, end, segment[2], speaker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 59.9, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [16.6, 17.5, 'SPEAKER_01', 'SPEAKER_01'],\n",
       " [17.7, 18.2, 'SPEAKER_01', 'SPEAKER_01'],\n",
       " [35.2, 35.7, 'SPEAKER_01', 'SPEAKER_01'],\n",
       " [50.0, 50.7, 'SPEAKER_01', 'SPEAKER_01'],\n",
       " [60.4, 147.4, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [147.8, 171.4, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [171.8, 201.4, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [201.9, 238.7, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [238.9, 287.7, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [243.4, 243.7, 'SPEAKER_01', 'SPEAKER_01'],\n",
       " [254.0, 254.6, 'SPEAKER_01', 'SPEAKER_01'],\n",
       " [273.1, 273.6, 'SPEAKER_01', 'SPEAKER_01'],\n",
       " [289.0, 297.6, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [298.2, 310.3, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [310.8, 361.6, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [361.9, 372.7, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [373.1, 417.1, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [417.3, 421.4, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [422.0, 431.4, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [432.0, 440.1, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [440.3, 441.2, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [441.4, 489.7, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [484.1, 484.6, 'SPEAKER_01', 'SPEAKER_01'],\n",
       " [489.8, 502.7, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [496.1, 497.0, 'SPEAKER_01', 'SPEAKER_01'],\n",
       " [503.3, 540.3, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [536.7, 537.4, 'SPEAKER_01', 'SPEAKER_01'],\n",
       " [540.7, 542.1, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [542.8, 547.9, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [546.1, 546.9, 'SPEAKER_01', 'SPEAKER_01'],\n",
       " [548.8, 553.0, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [553.3, 628.9, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [629.0, 643.8, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [644.1, 653.5, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [653.9, 665.3, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [666.4, 730.1, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [730.8, 767.5, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [768.3, 772.0, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [772.4, 774.7, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [775.1, 777.7, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [778.2, 782.4, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [782.9, 785.7, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [786.1, 786.7, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [786.8, 791.9, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [792.3, 795.3, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [795.5, 795.8, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [795.9, 798.3, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [798.7, 803.1, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [803.5, 806.5, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [806.7, 809.0, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [809.4, 816.0, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [816.4, 821.0, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [821.6, 823.7, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [824.3, 826.2, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [826.5, 828.4, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [828.7, 832.1, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [833.5, 834.0, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [834.2, 846.0, 'SPEAKER_00', 'SPEAKER_00'],\n",
       " [846.1, 987.8, 'SPEAKER_01', 'SPEAKER_01']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
